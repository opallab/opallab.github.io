- title: "Optimality of Message-Passing Architectures for Sparse Graphs"
  image: optimal_arch.jpg
  description: We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is O(1) in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical convolution in the regime of high graph signal. Furthermore, we prove a corresponding non-asymptotic result.
  authors: A. Baranwal, K. Fountoulakis, A. Jagannath
  link:
    url: https://arxiv.org/abs/2305.10391
    display: arXiv:2305.10391
  highlight: 1
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/2305.10391
    display: (Preprint)

- title: "Weighted flow diffusion for local graph clustering with node attributes: an algorithm and statistical guarantees"
  image: big_flow.jpg
  description: Local graph clustering methods aim to detect small clusters in very large graphs without the need to process the whole graph. They are fundamental and scalable tools for a wide range of tasks such as local community detection, node ranking and node embedding. While prior work on local graph clustering mainly focuses on graphs without node attributes, modern real-world graph datasets typically come with node attributes that provide valuable additional information. We present a simple local graph clustering algorithm for graphs with node attributes, based on the idea of diffusing mass locally in the graph while accounting for both structural and attribute proximities. Using high-dimensional concentration results, we provide statistical guarantees on the performance of the algorithm for the recovery of a target cluster with a single seed node. We give conditions under which a target cluster generated from a fairly general contextual random graph model, which includes both the stochastic block model and the planted cluster model as special cases, can be fully recovered with bounded false positives. Empirically, we validate all theoretical claims using synthetic data, and we show that incorporating node attributes leads to superior local clustering performances using real-world graph datasets.
  authors: S. Yang, K. Fountoulakis
  link:
    url: https://proceedings.mlr.press/v202/yang23d/yang23d.pdf
    display: ICML 2023, Oral
  highlight: 1
  news2:
  code:
    url: https://github.com/s-h-yang/WFD
    display: (Code)
  slides:
    url: https://cs.uwaterloo.ca/~s286yang/pdf/wfd-icml23.pdf
    display: (Slides)
  video:
    url: https://www.youtube.com/watch?v=BcJd15bMACs
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/2301.13187
    display: (Preprint)
    
- title: "On Classification Thresholds for Graph Attention with Edge Features"
  image: clean_edges_varying_q_positive_result.png
  description: The recent years we have seen the rise of graph neural networks for prediction tasks on graphs. One of the dominant architectures is graph attention due to its ability to make predictions using weighted edge features and not only node features. In this paper we analyze, theoretically and empirically, graph attention networks and their ability of correctly labelling nodes in a classic classification task. More specifically, we study the performance of graph attention on the classic contextual stochastic block model (CSBM). In CSBM the nodes and edge features are obtained from a mixture of Gaussians and the edges from a stochastic block model. We consider a general graph attention mechanism that takes random edge features as input to determine the attention coefficients. We study two cases, in the first one, when the edge features are noisy, we prove that the majority of the attention coefficients are up to a constant uniform. This allows us to prove that graph attention with edge features is not better than simple graph convolution for achieving perfect node classification. Second, we prove that when the edge features are clean graph attention can distinguish intra- from inter-edges and this makes graph attention better than classic graph convolution.
  authors: K. Fountoulakis, D. He, S. Lattanzi, B. Perozzi, A. Tsitsulin, S. Yang
  link:
    url: https://arxiv.org/abs/2210.10014
    display: arXiv:2210.10014
  highlight: 0
  news2:
  code:
    url: https://github.com/opallab/on-classification-thresholds-for-graph-attention-with-edge-features/tree/main
    display: (Code)
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/2210.10014
    display: (Preprint)

- title: "Effects of Graph Convolutions in Deep Networks"
  image: effects_of_gc_in_deep_networks.png
  description: Graph Convolutional Networks (GCNs) are one of the most popular architectures that are used to solve classification problems accompanied by graphical information. We present a rigorous theoretical understanding of the effects of graph convolutions in multi-layer networks. We study these effects through the node classification problem of a non-linearly separable Gaussian mixture model coupled with a stochastic block model. First, we show that a single graph convolution expands the regime of the distance between the means where multi-layer networks can classify the data by a factor of at least 1/(E[deg])^(1/4), where E[deg] denotes the expected degree of a node. Second, we show that with a slightly stronger graph density, two graph convolutions improve this factor to at least 1/n^(1/4), where n is the number of nodes in the graph. Finally, we provide both theoretical and empirical insights into the performance of graph convolutions placed in different combinations among the layers of a neural network, concluding that the performance is mutually similar for all combinations of the placement. We present extensive experiments on both synthetic and real-world data that illustrate our results.
  authors: A. Baranwal, K. Fountoulakis, A. Jagannath
  link:
    url: https://openreview.net/pdf?id=P-73JPgRs0R
    display: ICLR 2023, notable-top-25%
  highlight: 1
  news2:
  code:
    url: https://github.com/opallab/Effects-of-Graph-Convs-in-Deep-Nets
    display: (Code)
  slides:
    url:
    display:
  video:
    url: https://www.youtube.com/watch?v=AtsaBVu-Oq0
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/2204.09297
    display: (Preprint)

- title: "Graph Attention Retrospective"
  image: gammas_MLPGAT_varying_distance_q_04.png
  description: Graph-based learning is a rapidly growing sub-field of machine learning with applications in social networks, citation networks, and bioinformatics. One of the most popular type of models is graph attention networks. These models were introduced to allow a node to aggregate information from the features of neighbor nodes in a non-uniform way in contrast to simple graph convolution which does not distinguish the neighbors of a node. In this paper, we study theoretically this expected behaviour of graph attention networks. We prove multiple results on the performance of the graph attention mechanism for the problem of node classification for a contextual stochastic block model. Here the features of the nodes are obtained from a mixture of Gaussians and the edges from a stochastic block model where the features and the edges are coupled in a natural way. First, we show that in an "easy" regime, where the distance between the means of the Gaussians is large enough, graph attention maintains the weights of intra-class edges and significantly reduces the weights of the inter-class edges. As a corollary, we show that this implies perfect node classification independent of the weights of inter-class edges. However, a classical argument shows that in the "easy" regime, the graph is not needed at all to classify the data with high probability. In the "hard" regime, we show that every attention mechanism fails to distinguish intra-class from inter-class edges. We evaluate our theoretical results on synthetic and real-world data.
  authors: K. Fountoulakis, A. Levi, S. Yang, A. Baranwal, A. Jagannath
  link:
    url: https://jmlr.org/papers/v24/22-125.html
    display: Journal of Machine Learning Resarch
  highlight: 1
  news2: Best paper award at the <a href='https://sites.google.com/view/groundedml2022'>GroundedML</a> workshop at ICLR 2022.
  code:
    url: https://github.com/opallab/Graph-Attention-Retrospective/
    display: (Code)
  slides:
    url:
    display:
  video:
    url: https://www.youtube.com/watch?v=duWVNO8_sDM
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/2202.13060
    display: (Preprint)
    
- title: "Local Hyper-flow Diffusion"
  image: local_hyper_flow.jpg
  description: A plethora of real-world problems require utilization of hypergraphs and diffusion algorithms. Examples include recommendation systems, node ranking in food networks and community detection in social networks to mention a few. Due to the increased size and complexity of real hypergraphs, local and accurate diffusion algorithms that work with the most complex hypergraphs are in need. We propose the first local diffusion method that works on higher-order relations with only a submodularity assumption. Our method is based on a primal-dual optimization formulation where the primal problem has a natural network flow interpretation, and the dual problem has a cut-based interpretation using the l2-norm penalty for general submodular cut-costs. We prove that the proposed formulation achieves quadratic approximation error for the problem of local hypergraph clustering. We demonstrate that the new technique is significantly better than state-of-the-art methods over a range of real datasets for the local hypergraph clustering and node ranking problems.
  authors: K. Fountoulakis, P. Li, S. Yang
  link:
    url: https://proceedings.neurips.cc/paper/2021/hash/e924517087669cf201ea91bd737a4ff4-Abstract.html
    display: Conference on Neural Information Processing Systems (NeurIPS) 2021
  highlight: 0
  news2:
  code:
    url: https://github.com/opallab/Local-Hyper-flow-Diffusion
    display: (Code)
  slides:
    url:
    display:
  video:
    url: https://www.youtube.com/watch?v=CcOIw78zQqw
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/2102.07945
    display: (Preprint)
  
- title: "Graph Convolution for Semi-Supervised Classification: Improved Linear Separability and Out-of-Distribution Generalization"
  image: after-conv.png
  description: Recently there has been increased interest in semi-supervised classification in the presence of graphical information. A new class of learning models has emerged that relies, at its most basic level, on classifying the data after first applying a graph convolution. To understand the merits of this approach, we study the classification of a mixture of Gaussians, where the data corresponds to the node attributes of a stochastic block model. We show that graph convolution extends the regime in which the data is linearly separable by a factor of roughly 1/sqrt{D}, where D is the expected degree of a node, as compared to the mixture model data on its own. Furthermore, we find that the linear classifier obtained by minimizing the cross-entropy loss after the graph convolution generalizes to out-of-distribution data where the unseen data can have different intra- and inter-class edge probabilities from the training data.
  authors: A. Baranwal, K. Fountoulakis, A. Jagannath
  link:
    url: http://proceedings.mlr.press/v139/baranwal21a.html
    display: International Conference on Machine Learning (ICML) 2021 (spotlight)
  highlight: 0
  news2:
  code:
    url: https://github.com/opallab/Graph-Convolution-for-Semi-Supervised-Classification-Improved-Linear-Separability-and-OoD-Gen.
    display: (Code)
  slides:
    url:
    display:
  video:
    url: https://zoom.us/rec/play/X1FbBJiP1bLTixjmU7wBw233sutk939XulBkrY0Szes9KSNh_cFovdohKoZ-KXFaCZJ5G5yg4m5nKZol.47Ol60UmzMVLZec8?startTime=1624287370000&_x_zm_rtaid=2ArAs6KUSwiFwnA5V61cmQ.1624792221031.e0fb3030146eeed7cee824bfc92e70b5&_x_zm_rhtaid=77
    display: (Video, time 1:03:34)
  preprint:
    url: https://arxiv.org/abs/2102.06966
    display: (Preprint)
  
- title: "Targeted Pandemic Containment Through Identifying Local Contact Network Bottlenecks"
  image: targeted_pandemic.jpg
  description: Decision-making about pandemic mitigation often relies upon mathematical modelling. Models of contact networks are increasingly used for these purposes, and are often appropriate for infections that spread from person to person. Real-world human contact networks are rich in structural features that influence infection transmission, such as tightly-knit local communities that are weakly connected to one another. In this paper, we propose a new flow-based edge-betweenness centrality method for detecting bottleneck edges that connect communities in contact networks. In particular, we utilize convex optimization formulations based on the idea of diffusion with p-norm network flow. Using mathematical models of COVID-19 transmission through real network data at both individual and county levels, we demonstrate that targeting bottleneck edges identified by the proposed method reduces the number of infected cases by up to 10% more than state-of-the-art edge-betweenness methods. Furthermore, we demonstrate empirically that the proposed method is orders of magnitude faster than existing methods.
  authors: S. Yang, P. Senapati, D. Wang, C. T. Bauch, K. Fountoulakis
  link:
    url: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009351
    display: PLOS Computational Biology
  highlight: 0
  news2:
  code:
    url: https://github.com/opallab/TargetedPandemicContainment
    display: (Code)
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/2006.06939
    display: (Preprint)

- title: "p-Norm Flow Diffusion for Local Graph Clustering"
  image: p_norm.jpg
  description: Local graph clustering and the closely related seed set expansion problem are primitives on graphs that are central to a wide range of analytic and learning tasks such as local clustering, community detection, nodes ranking and feature inference. Prior work on local graph clustering mostly falls into two categories with numerical and combinatorial roots respectively. In this work, we draw inspiration from both fields and propose a family of convex optimization formulations based on the idea of diffusion with p-norm network flow for p in (1,infinity). In the context of local clustering, we characterize the optimal solutions for these optimization problems and show their usefulness in finding low conductance cuts around input seed set. In particular, we achieve quadratic approximation of conductance in the case of p=2 similar to the Cheeger-type bounds of spectral methods, constant factor approximation when p goes to infinity similar to max-flow based methods, and a smooth transition for general p values in between. Thus, our optimization formulation can be viewed as bridging the numerical and combinatorial approaches, and we can achieve the best of both worlds in terms of speed and noise robustness. We show that the proposed problem can be solved in strongly local running time for p greater or equal to 2 and conduct empirical evaluations on both synthetic and real-world graphs to illustrate our approach compares favorably with existing methods.
  authors: K. Fountoulakis, D. Wang, S. Yang
  link:
    url: https://proceedings.mlr.press/v119/fountoulakis20a.html
    display: International Conference on Machine Learning (ICML) 2020
  highlight: 0
  news2:
  code:
    url: https://github.com/opallab/pNormFlowDiffusion
    display: (Code)
  slides:
    url:
    display:
  video:
    url: https://www.youtube.com/watch?v=X6V11ZFCkk8&feature=emb_title
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/2005.09810
    display: (Preprint)

- title: "Flow-based Algorithms for Improving Clusters: A Unifying Framework, Software, and Performance"
  image: big_flow.jpg
  description: Clustering points in a vector space or nodes in a graph is a ubiquitous primitive in statistical data analysis, and it is commonly used for exploratory data analysis. In practice, it is often of interest to refine or improve a given cluster that has been obtained by some other method. In this survey, we focus on principled algorithms for this cluster improvement problem. Many such cluster improvement algorithms are flow-based methods, by which we mean that operationally they require the solution of a sequence of maximum flow problems on a (typically implicitly) modified data graph. These cluster improvement algorithms are powerful, both in theory and in practice, but they have not been widely adopted for problems such as community detection, local graph clustering, semi-supervised learning, etc. Possible reasons for this are the steep learning curve for these algorithms; the lack of efficient and easy to use software; and the lack of detailed numerical experiments on real-world data that demonstrate their usefulness. Our objective here is to address these issues. To do so, we guide the reader through the whole process of understanding how to implement and apply these powerful algorithms. We present a unifying fractional programming optimization framework that permits us to distill out in a simple way the crucial components of all these algorithms. It also makes apparent similarities and differences between related methods. Viewing these cluster improvement algorithms via a fractional programming framework suggests directions for future algorithm development. Finally, we develop efficient implementations of these algorithms in our LocalGraphClustering python package, and we perform extensive numerical experiments to demonstrate the performance of these methods on social networks and image-based data graphs.
  authors: K. Fountoulakis, M. Liu, D. F. Gleich, M. W. Mahoney
  link:
    url: https://epubs.siam.org/doi/abs/10.1137/20M1333055
    display:  SIAM Review
  highlight: 0
  news2:
  code:
    url: https://github.com/opallab/LocalGraphClustering
    display: (Code)
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/2004.09608
    display: (Preprint)
  
- title: "Statistical Guarantees for Local Graph Clustering (Long version at JMLR)"
  image:
  description:
  authors: W. Ha, K. Fountoulakis, M. W. Mahoney
  link:
    url: https://jmlr.org/papers/v22/20-029.html
    display:  Journal of Machine Learning Research
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1906.04863
    display: (Preprint)
  
- title: "Statistical Guarantees for Local Graph Clustering"
  image:
  description:
  authors: W. Ha, K. Fountoulakis, M. W. Mahoney
  link:
    url: http://proceedings.mlr.press/v108/ha20a.html
    display:  International Conference on Artificial Intelligence and Statistics (AISTATS) 2020
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1906.04863
    display: (Preprint)

- title: "Parallel and Communication Avoiding Least Angle Regression"
  image:
  description:
  authors: S. Das, J. Demmel, K. Fountoulakis, L. Grigori, M. W. Mahoney, S. Yang
  link:
    url: https://epubs.siam.org/doi/abs/10.1137/19M1305720
    display:  SIAM Journal on Scientific Computing
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1905.11340
    display: (Preprint)

- title: "Locality And Structure Aware Graph Node Embedding"
  image:
  description:
  authors: E. Faerman, F. Borutta, K. Fountoulakis, M. W. Mahoney
  link:
    url: https://ieeexplore.ieee.org/document/8609600
    display: International Conference on Web Intelligence 2018 (Best student paper award)
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1710.06520
    display: (Preprint)

- title: "A Flexible Coordinate Descent Method"
  image:
  description:
  authors: K. Fountoulakis, R. Tappenden
  link:
    url: https://link.springer.com/article/10.1007/s10589-018-9984-3?wt_mc:Internal.Event.1.SEM.ArticleAuthorOnlineFirst
    display: Computational Optimization and Applications
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1507.03713
    display: (Preprint)

- title: "Avoiding Synchronization in First-Order Methods for Sparse Convex Optimization"
  image:
  description:
  authors: A. Devarakonda, K. Fountoulakis, J. Demmel, M. Mahoney
  link:
    url: https://ieeexplore.ieee.org/document/8425195/?part:1
    display:  International Parallel and Distributed Processing Symposium (IPDPS) 2018
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url: https://www.youtube.com/watch?v=uElfprD9Tb8
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/1712.06047
    display: (Preprint)

- title: "Variational Perspective on Local Graph Clustering"
  image:
  description:
  authors: K. Fountoulakis, F. Roosta-Khorasani, J. Shun, X. Cheng, M. Mahoney
  link:
    url: https://link.springer.com/article/10.1007/s10107-017-1214-8
    display:  Mathematical Programming
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url: https://www.youtube.com/watch?v=uElfprD9Tb8&t=1241s
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/1602.01886
    display: (Preprint)

- title: "Capacity Releasing Diffusion for Speed and Locality"
  image:
  description:
  authors: D. Wang, K. Fountoulakis, M. Henzinger, M. Mahoney, S. Rao
  link:
    url: http://proceedings.mlr.press/v70/wang17b.html
    display: International Conference on Machine Learning (ICML) 2017
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url: https://vimeo.com/238227621
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/1706.05826
    display: (Preprint)

- title: "Avoiding communication in primal and dual block coordinate descent methods"
  image:
  description:
  authors: A. Devarakonda, K. Fountoulakis, J. Demmel, M. Mahoney
  link:
    url: https://epubs.siam.org/doi/abs/10.1137/17M1134433?casa_token:gTnl0wYpllAAAAAA:zWbDKDmOMWt-XbIzK4XGMj3mzEUS1s6FutK9Z0SUP_mY6MdQHGsOobO4eIiAxj-Yb05XVRHCIYY
    display: SIAM Journal on Scientific Computing (SISC)
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url: https://www.youtube.com/watch?v=uElfprD9Tb8
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/1612.04003
    display: (Preprint)
  
- title: "A Randomized Rounding Algorithm for Sparse PCA"
  image:
  description:
  authors: K. Fountoulakis, A. Kundu, E. M. Kontopoulou, P. Drineas
  link:
    url: https://dl.acm.org/doi/10.1145/3046948
    display: ACM Transactions on Knowledge Discovery from Data
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1508.03337
    display: (Preprint)

- title: "An Optimization Approach to Locally-Biased Graph Algorithms"
  image:
  description:
  authors: K. Fountoulakis, D. Gleich, M. Mahoney
  link:
    url: https://ieeexplore.ieee.org/document/7828195/
    display: Proceedings of the IEEE
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url: https://www.youtube.com/watch?v=mf9h1-jhyos
    display: (Video)
  preprint:
    url: https://arxiv.org/abs/1607.04940
    display: (Preprint)

- title: "Performance of First- and Second-Order Methods for L1-Regularized Least Squares Problems"
  image:
  description:
  authors: K. Fountoulakis, J. Gondzio
  link:
    url: https://link.springer.com/article/10.1007/s10589-016-9853-x
    display: Computational Optimization and Applications
  highlight: 0
  news2:
  code:
    url: https://www.maths.ed.ac.uk/ERGO/trillion/
    display: (Code)
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1306.5386
    display: (Preprint)

- title: "A Second-Order Method for Strongly-Convex L1-Regularization Problems"
  image:
  description:
  authors: K. Fountoulakis, J. Gondzio
  link:
    url: https://link.springer.com/article/10.1007%2Fs10107-015-0875-4
    display: Mathematical Programming
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1306.5386
    display: (Preprint)

- title: "Parallel Local Graph Clustering"
  image:
  description:
  authors: J. Shun, F. Roosta-Khorasani, K. Fountoulakis, M. Mahoney
  link:
    url: https://dl.acm.org/doi/10.14778/2994509.2994522
    display: Proceedings of the VLDB Endowment (VLDB) 2016
  highlight: 0
  news2:
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1604.07515
    display: (Preprint)

- title: "A Preconditioner for a Primal-dual Newton Conjugate Gradients Method for Compressed Sensing Problems"
  image:
  description:
  authors: I. Dassios, K. Fountoulakis, J. Gondzio
  link:
    url: https://epubs.siam.org/doi/abs/10.1137/141002062
    display: SIAM Journal on Scientific Computing (SISC)
  highlight: 0
  news2:
  code:
    url: https://www.maths.ed.ac.uk/ERGO/pdNCG/
    display: (Code)
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1501.00047
    display: (Preprint)

- title: "Matrix-free interior point method for compressed sensing problems"
  image:
  description:
  authors: K. Fountoulakis, J. Gondzio, P. Zhlobich
  link:
    url: https://link.springer.com/article/10.1007/s12532-013-0063-6
    display: Mathematical Programming Computation
  highlight: 0
  news2:
  code:
    url: https://www.maths.ed.ac.uk/ERGO/mfipmcs/
    display: (Code)
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url: https://arxiv.org/abs/1208.5435
    display: (Preprint)

- title: "Higher-Order Methods for Large-Scale Optimization"
  image:
  description:
  authors: K. Fountoulakis
  link:
    url: https://era.ed.ac.uk/handle/1842/15797
    display: Kimon Fountoulakis' PhD Thesis
  highlight: 0
  code:
    url:
    display:
  slides:
    url:
    display:
  video:
    url:
    display:
  preprint:
    url:
    display:
  news2:
